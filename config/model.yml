model:
  type: MLP
  hidden_layers: [64, 64, 32]
  activation: relu
  dropout_ranges:
    - [0.2, 0.3]
    - [0.2, 0.3]
    - [0.3, 0.4]
